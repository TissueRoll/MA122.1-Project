\documentclass[a4paper, 11pt, english]{article}
\usepackage[margin=0.8in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage[american, siunitx]{circuitikz}
\usepackage{tikz-timing}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{color}
\usepackage[backend=biber]{biblatex}
\addbibresource{ref.bib}
% \usepackage{etoolbox}
% \apptocmd{\sloppy}{\hbadness 10000\relax}{}{}
\lstset
{
  language = {},
  basicstyle = \scriptsize,
  numbers = left,
  stepnumber = 1,
  showstringspaces = false,
  frame = single,
  tabsize = 1,
  breaklines = true,
  breakatwhitespace = false,
}
\pagestyle{fancy}
\lhead{Christopher Dizon \& Nate Kibanoff \& Sir Heinrich Tan\\CS 152B B}
\rhead{MA122.1 Project\\Neural Networks}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\renewcommand\qedsymbol{QED}
% \renewcommand\qedsymbol{$\blacksquare$}
%from external sources
%--------------------------------------------------------------------------------------
%https://tex.stackexchange.com/questions/235118/making-a-thicker-cdot-for-dot-product-that-is-thinner-than-bullet/235120 (Manuel)
\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother
%--------------------------------------------------------------------------------------

\begin{document}
\section{Introduction}
% intro to neural networks
% provide pictures
% example to put in picture
% \begin{figure}[H]
%   \centering
%   \includegraphics[width=12cm]{filename (in the same folder dapat)}
%   \caption{caption here}
%   \label{fig:label of the figure}
% \end{figure}
Neural networks have a lot of applications in today's world. They have applications in character recognition, image compression, stock market prediction, medicine, and much more \cite{dk1}. They provide us with a way to compute for more complicated things. % pls edit with more pictures
\par When neural networks are mentioned, people tend to panic as they think it is a very complicated topic. However, it is not incredibly complicated. For people with a background in linear algebra, neural networks become something more tangible, because it is just applying concepts in linear algebra.
\par For this topic, we will discuss a basic kind of neural network. It is not too complicated (like the modern versions of neural networks) and it is easy to follow.

\section{Preliminaries}
% activation functions/sigmoid
\subsection{Activation Function}
\par The activation function is simply a function that squeezes numbers to fit into a certain range. In this case, we try to fit numbers in the range $[0,1]$. There are many kinds of activation functions, and the function we use for this topic is the sigmoid function. The sigmoid function is defined as
\[\mathrm{sigmoid}(x) = \frac{1}{1+e^{-x}}.\]

% gradient descent
\subsection{Gradient Descent}
\par Gradient descent is a method to get the minimum of a function. % lmao idk na

% include bias?

\subsection{Definition of Terms}
\par Before discussing how the neural network works, we must define the following terms:
\begin{description}
    \item[Neurons] \hfill \\ These are represented as the nodes in the neural network.
    \item[Layers] \hfill \\ These are the vertical arrangements of neurons in the neural network.
    \item[Weights] \hfill \\ These are represented as the edges  connecting one neuron to the all neurons in the next layer.
\end{description}

\section{Discussion}
% how is linear alg used
% forward phase

% checking for error

% propagate the error backwards (backward phase)

% training the network

% arriving at an answer

% sample

\section{Results}
% results of code?

\section{Conclusion}
% summarize everything

\newpage
\printbibliography
\end{document}
